{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNeKQ46ti8XyrOPAAXviPl7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "We propose a semi-supervised learning method for tabular data that does not require any domain-specific assumption. The method we propose is based on pseudo-labeling of a set of unlabeled data using Confident Sinkhorn Allocation (CSA). Our method is theoretically driven by the role of uncertainty in robust label assignment in SSL. CSA will assign labels to only the data samples with high confidence scores using Sinkhorn’s algorithm. By learning the label assignment with optimal transport, CSA eliminates the need to predefine the heuristic thresholds used in existing pseudo-labeling methods, which can be greedy. The proposed CSA is applicable to any data domain, and could be used in concert with consistency-based approaches, but is particularly useful for tabular data where pretext tasks and data augmentation are not applicable."
      ],
      "metadata": {
        "id": "P356XnK7eWeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/amzn/confident-sinkhorn-allocation.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxU1N1O8Kbzc",
        "outputId": "00c3c6f2-f22b-4eaa-ade4-b20d9005772c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'confident-sinkhorn-allocation'...\n",
            "remote: Enumerating objects: 2201, done.\u001b[K\n",
            "remote: Counting objects: 100% (76/76), done.\u001b[K\n",
            "remote: Compressing objects: 100% (66/66), done.\u001b[K\n",
            "remote: Total 2201 (delta 40), reused 22 (delta 10), pack-reused 2125\u001b[K\n",
            "Receiving objects: 100% (2201/2201), 52.93 MiB | 12.62 MiB/s, done.\n",
            "Resolving deltas: 100% (864/864), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd confident-sinkhorn-allocation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCZhyxZbKuT-",
        "outputId": "04936868-4e08-4de9-c73d-1c9b007c7e95"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/confident-sinkhorn-allocation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "QJKToONVMG_6",
        "outputId": "cb68e9fc-fd81-44a6-cc9d-85f68102fd9b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.23.0\n",
            "  Downloading numpy-1.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chex 0.1.86 requires numpy>=1.24.1, but you have numpy 1.23.0 which is incompatible.\n",
            "pandas-stubs 2.0.3.230814 requires numpy>=1.25.0; python_version >= \"3.9\", but you have numpy 1.23.0 which is incompatible.\n",
            "tensorflow 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 1.23.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "26c4139543c0494599f2046575878eba"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import argparse\n",
        "import logging\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "en-PXQ_VfeMJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TB5S6TFAePVP"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def str2num(s, encoder):\n",
        "    return encoder[s]\n",
        "\n",
        "def append_acc_early_termination(AccList, NumIter):\n",
        "\n",
        "    if len(AccList)<=NumIter:\n",
        "        Acc_Last_Iter=AccList[-1]\n",
        "        AccList = AccList + [Acc_Last_Iter]*(1+NumIter-len(AccList))\n",
        "\n",
        "    return AccList\n",
        "\n",
        "def rename_dataset(dataset_name):\n",
        "    print(dataset_name)\n",
        "    newname=[]\n",
        "\n",
        "    if dataset_name==\"madelon_no\":\n",
        "        return \"Madelon\"\n",
        "    elif dataset_name==\"synthetic_control_6c\":\n",
        "        return \"Synthetic Control\"\n",
        "    elif dataset_name==\"digits\":\n",
        "        return \"Digits\"\n",
        "    elif dataset_name==\"analcatdata_authorship\":\n",
        "        return \"Analcatdata\"\n",
        "    elif dataset_name==\"German-credit\":\n",
        "        return \"German Credit\"\n",
        "    elif dataset_name==\"segment_2310_20\":\n",
        "        return \"Segment\"\n",
        "    elif dataset_name==\"wdbc_569_31\":\n",
        "        return \"Wdbc\"\n",
        "    elif dataset_name==\"dna_no\":\n",
        "        return \"Dna\"\n",
        "    elif dataset_name==\"agaricus-lepiota\":\n",
        "        return \"Agaricus-Lepiota\"\n",
        "    elif dataset_name==\"breast_cancer\":\n",
        "        return \"Breast Cancer\"\n",
        "    elif dataset_name==\"agaricus-lepiota\":\n",
        "        return \"Agaricus-Lepiota\"\n",
        "    elif dataset_name==\"emotions\":\n",
        "        return \"Emotions\"\n",
        "\n",
        "\n",
        "# 18,7,6,4,2\n",
        "def get_train_test_unlabeled(_datasetName,path_to_data,random_state=0): # for multi-classification\n",
        "    \"\"\"\n",
        "    path_to_data='all_data.pickle'\n",
        "    \"\"\"\n",
        "\n",
        "    # load the data\n",
        "    with open(path_to_data, 'rb') as handle:\n",
        "        [all_data, datasetName_list] = pickle.load(handle)\n",
        "\n",
        "    dataset_index= datasetName_list.index(_datasetName)\n",
        "    data=all_data[dataset_index]\n",
        "\n",
        "    #if dataset_index<14:\n",
        "    if _datasetName in ['segment_2310_20','wdbc_569_31','steel-plates-fault','analcatdata_authorship','synthetic_control_6c',\\\n",
        "    'vehicle_846_19','German-credit','gina_agnostic_no','madelon_no','texture','gas_drift','dna_no']:\n",
        "        _dic = list(set(data.values[:, -1]))\n",
        "        num_labels = len(_dic)\n",
        "        encoder = {}\n",
        "        for i in range(len(_dic)):\n",
        "            encoder[_dic[i]] = i\n",
        "\n",
        "        # shuffle original dataset\n",
        "        data = data.sample(frac=1,random_state=42)\n",
        "        X = data.values[:, :-1]\n",
        "        # X = scale(X)  # scale the X\n",
        "        scaler = StandardScaler()\n",
        "        X = scaler.fit_transform(X)\n",
        "        Y = np.array([str2num(s, encoder) for s in data.values[:, -1]])\n",
        "    else:\n",
        "        X = data[:, :-1]\n",
        "        Y=data[:,-1]\n",
        "\n",
        "\n",
        "    #if dataset_index in [9,1,16]:\n",
        "    if _datasetName in ['hill-valley','gina_agnostic_no','agaricus-lepiota']:\n",
        "        x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2,\n",
        "                                                            random_state=random_state)\n",
        "\n",
        "        x_train, x_unlabeled, y_train, y_unlabeled = train_test_split(x_train, y_train,\n",
        "                                                      test_size=0.6, random_state=random_state)\n",
        "\n",
        "    #elif dataset_index in [17,8]:\n",
        "    elif _datasetName in ['German-credit','breast_cancer']:\n",
        "        x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2,\n",
        "                                                            random_state=random_state)\n",
        "\n",
        "        x_train, x_unlabeled, y_train, y_unlabeled = train_test_split(x_train, y_train,\n",
        "                                                      test_size=0.8, random_state=random_state)\n",
        "\n",
        "\n",
        "    #elif dataset_index in [18,6,4]:\n",
        "    elif _datasetName in ['steel-plates-fault','synthetic_control_6c','digits']:\n",
        "\n",
        "        x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2,\n",
        "                                                            random_state=random_state)\n",
        "\n",
        "        x_train, x_unlabeled, y_train, y_unlabeled = train_test_split(x_train, y_train,\n",
        "                                                      test_size=0.9, random_state=random_state)\n",
        "\n",
        "\n",
        "\n",
        "    #elif dataset_index in [10,15,12,14,11,13]: # label / unlabel > 15:1\n",
        "    elif _datasetName in ['madelon_no','texture','gas_drift','dna_no','car','kr_vs_kp']:\n",
        "        x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2,\n",
        "                                                            random_state=random_state)\n",
        "\n",
        "        x_train, x_unlabeled, y_train, y_unlabeled = train_test_split(x_train, y_train,\n",
        "                                                      test_size=0.94, random_state=random_state)\n",
        "\n",
        "    #elif dataset_index in [3,5]: # label / unlabel > 15:1\n",
        "    elif _datasetName in ['wdbc_569_31','analcatdata_authorship']:\n",
        "\n",
        "        x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2,\n",
        "                                                            random_state=random_state)\n",
        "\n",
        "        x_train, x_unlabeled, y_train, y_unlabeled = train_test_split(x_train, y_train,\n",
        "                                                      test_size=0.9, random_state=random_state)\n",
        "\n",
        "    #elif dataset_index in [7]:\n",
        "    elif _datasetName in ['vehicle_846_19']:\n",
        "\n",
        "        x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2,\n",
        "                                                            random_state=random_state)\n",
        "\n",
        "        x_train, x_unlabeled, y_train, y_unlabeled = train_test_split(x_train, y_train,\n",
        "                                                      test_size=0.8, random_state=random_state)\n",
        "\n",
        "    #elif dataset_index in [2]:\n",
        "    elif _datasetName in ['segment_2310_20']:\n",
        "        x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2,\n",
        "                                                            random_state=random_state)\n",
        "\n",
        "        x_train, x_unlabeled, y_train, y_unlabeled = train_test_split(x_train, y_train,\n",
        "                                                      test_size=0.7, random_state=random_state)\n",
        "    else:\n",
        "        print(_datasetName + \"is not defined. please check!\")\n",
        "\n",
        "\n",
        "    p = np.random.permutation(x_train.shape[0])\n",
        "    x_train, y_train = x_train[p], y_train[p]\n",
        "\n",
        "    p = np.random.permutation(x_unlabeled.shape[0])\n",
        "    x_unlabeled, y_unlabeled = x_unlabeled[p], y_unlabeled[p]\n",
        "\n",
        "    y_test=np.reshape(y_test,(-1,1))\n",
        "    y_train=np.reshape(y_train,(-1,1))\n",
        "\n",
        "\n",
        "    return x_train,y_train, x_test, y_test, x_unlabeled\n",
        "\n",
        "\n",
        "def get_train_test_unlabeled_for_multilabel(_datasetName,path_to_data='all_data_multilabel.pickle',random_state=0): # for multi-label classification\n",
        "    \"\"\"\n",
        "    path_to_data='all_data.pickle'\n",
        "    \"\"\"\n",
        "\n",
        "    # load the data\n",
        "    with open(path_to_data, 'rb') as handle:\n",
        "        [all_data, datasetName_list] = pickle.load(handle)\n",
        "\n",
        "    dataset_index= datasetName_list.index(_datasetName)\n",
        "    data=all_data[dataset_index]\n",
        "\n",
        "    X = data['data']\n",
        "    Y=data['target']\n",
        "\n",
        "\n",
        "    if _datasetName==\"emotions\": # emotions dataset\n",
        "        x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=random_state)\n",
        "\n",
        "        x_train, x_unlabeled, y_train, y_unlabeled = train_test_split(x_train, y_train,\n",
        "                                                        test_size=0.5, random_state=random_state)\n",
        "    elif _datasetName==\"genbase\": # genbase dataset\n",
        "        x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=random_state)\n",
        "\n",
        "        x_train, x_unlabeled, y_train, y_unlabeled = train_test_split(x_train, y_train,\n",
        "                                                        test_size=0.7, random_state=random_state)\n",
        "    elif _datasetName==\"yeast\": # yeast dataset\n",
        "        x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=random_state)\n",
        "\n",
        "        x_train, x_unlabeled, y_train, y_unlabeled = train_test_split(x_train, y_train,\n",
        "                                                        test_size=0.7, random_state=random_state)\n",
        "    else:\n",
        "        print(_datasetName + \"is not defined. please check!\")\n",
        "\n",
        "\n",
        "    p = np.random.permutation(x_train.shape[0])\n",
        "    x_train, y_train = x_train[p], y_train[p]\n",
        "\n",
        "    p = np.random.permutation(x_unlabeled.shape[0])\n",
        "    x_unlabeled, y_unlabeled = x_unlabeled[p], y_unlabeled[p]\n",
        "\n",
        "    return x_train,y_train, x_test, y_test, x_unlabeled"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "import copy\n",
        "import sklearn\n",
        "\n",
        "class Pseudo_Labeling(object):\n",
        "    # implementation of the master class for pseudo-labeling\n",
        "    # this class will be inherited across other subclasses\n",
        "\n",
        "    def __init__(self, unlabelled_data, x_test,y_test, num_iters=5,upper_threshold = 0.8, \\\n",
        "            fraction_allocation=1,lower_threshold = None,num_XGB_models=0, \\\n",
        "                 verbose = False,IsMultiLabel=False):\n",
        "        \"\"\"\n",
        "        unlabelled_data      : [N x d] where N is the number of unlabeled data, d is the feature dimension\n",
        "        x_test               :[N_test x d]\n",
        "        y_test               :[N_test x 1] for multiclassification or [N_test x K] for multilabel classification\n",
        "        num_iters            : number of pseudo-iterations, recommended = 5 as in the paper\n",
        "        upper_threshold      : the upper threshold used for pseudo-labeling, e.g., we assign label if the prob > 0.8\n",
        "        fraction_allocation  : the faction of label allocation, if fraction_allocation=1, we assign labels to 100% of unlabeled data\n",
        "        lower_threshold      : lower threshold, used for UPS\n",
        "        num_XGB_models       : number of XGB models used for UPS and CSA, recommended = 10\n",
        "        verbose              : verbose\n",
        "        IsMultiLabel         : False => Multiclassification or True => Multilabel classification\n",
        "        \"\"\"\n",
        "\n",
        "        self.IsMultiLabel=False\n",
        "        self.algorithm_name=\"Pseudo_Labeling\"\n",
        "        self.x_test=x_test\n",
        "        self.y_test=y_test\n",
        "\n",
        "        self.IsMultiLabel=IsMultiLabel\n",
        "\n",
        "        # for house keeping and reporting purpose\n",
        "        self.len_unlabels=[]\n",
        "        self.len_accepted_ttest=[]\n",
        "        self.len_selected=[]\n",
        "        self.num_augmented_per_class=[]\n",
        "\n",
        "\n",
        "        # this is the XGBoost model for multi-class classification\n",
        "        param = {}\n",
        "        param['booster'] = 'gbtree'\n",
        "        param['objective'] = 'binary:logistic'\n",
        "        param['verbosity'] = 0\n",
        "        param['silent'] = 1\n",
        "        param['seed'] = 0\n",
        "\n",
        "        # create XGBoost instance with default hyper-parameters\n",
        "        #xgb = XGBClassifier(**param,use_label_encoder=False)\n",
        "        xgb = self.get_XGB_model(param)\n",
        "\n",
        "        self.model = copy.copy(xgb)\n",
        "\n",
        "        self.unlabelled_data = unlabelled_data # this is a temporary unlabelled data changing in each iteration\n",
        "        self.verbose = verbose\n",
        "        self.upper_threshold = upper_threshold\n",
        "        self.num_iters=num_iters\n",
        "\n",
        "        if lower_threshold is not None:\n",
        "            self.lower_threshold = lower_threshold # this lower threshold is used for UPS algorithm, not the vanilla Pseudo-labeling\n",
        "\n",
        "\n",
        "        # allow the pseudo-data is repeated, e.g., without removing them after each iteration\n",
        "        # create a list of all the indices\n",
        "        self.unlabelled_indices = list(range(unlabelled_data.shape[0]))\n",
        "\n",
        "        self.selected_unlabelled_index=[]\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"no of unlabelled data:\",unlabelled_data.shape[0], \"\\t no of test data:\",x_test.shape[0])\n",
        "\n",
        "        # Shuffle the indices\n",
        "        np.random.shuffle(self.unlabelled_indices)\n",
        "        self.test_acc=[]\n",
        "        self.FractionAllocatedLabel=fraction_allocation # we will allocate labels to 100% of the unlabeled dataset\n",
        "        self.num_XGB_models=num_XGB_models # this is the parameter M in our paper\n",
        "\n",
        "        if num_XGB_models>1: # will be used for CSA and UPS\n",
        "            # for uncertainty estimation\n",
        "            # generate multiple models\n",
        "            params = { 'max_depth': np.arange(3, 20).astype(int),\n",
        "                    'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
        "                    'subsample': np.arange(0.5, 1.0, 0.05),\n",
        "                    'colsample_bytree': np.arange(0.4, 1.0, 0.05),\n",
        "                    'colsample_bylevel': np.arange(0.4, 1.0, 0.05),\n",
        "                    'n_estimators': [100, 200, 300, 500, 600, 700, 1000]}\n",
        "\n",
        "            self.XGBmodels_list=[0]*self.num_XGB_models\n",
        "\n",
        "            param_list=[0]*self.num_XGB_models\n",
        "            for tt in range(self.num_XGB_models):\n",
        "\n",
        "                param_list[tt]={}\n",
        "\n",
        "                for key in params.keys():\n",
        "\n",
        "                    mychoice=np.random.choice(params[key])\n",
        "\n",
        "                    param_list[tt][key]=mychoice\n",
        "                    param_list[tt]['verbosity'] = 0\n",
        "                    param_list[tt]['silent'] = 1\n",
        "                    param_list[tt]['seed'] = tt\n",
        "\n",
        "                #self.XGBmodels_list[tt] = XGBClassifier(**param_list[tt],use_label_encoder=False)\n",
        "                self.XGBmodels_list[tt] = self.get_XGB_model(param_list[tt])\n",
        "\n",
        "\n",
        "    def get_XGB_model(self,param):\n",
        "        \"\"\"\n",
        "        we create the XGB model depending on multiclass or multi-label setting\n",
        "        Args:\n",
        "            param: a predefined hyperparameter for XGBmodel\n",
        "\n",
        "        Output:\n",
        "            a single XGBClassifier for multiclass\n",
        "            or\n",
        "            a single MultiOutputClassifier for multilabel\n",
        "        \"\"\"\n",
        "\n",
        "        if self.IsMultiLabel==False:\n",
        "            return XGBClassifier(**param,use_label_encoder=False)\n",
        "        else:\n",
        "            return MultiOutputClassifier(XGBClassifier(**param,use_label_encoder=False))\n",
        "\n",
        "    def get_predictive_prob_for_unlabelled_data(self, model):\n",
        "        \"\"\"\n",
        "        Compute the predictive probability within [0,1] for unlabelled data given a single XGB model\n",
        "        Args:\n",
        "            model: a single XGBmodel\n",
        "\n",
        "        Output:\n",
        "            predictive probability matrix [N x K]\n",
        "        \"\"\"\n",
        "\n",
        "        pseudo_labels_prob = model.predict_proba(self.unlabelled_data)\n",
        "\n",
        "        # number of unlabeled data\n",
        "        if self.IsMultiLabel==True:\n",
        "            pseudo_labels_prob=np.asarray(pseudo_labels_prob).T\n",
        "            pseudo_labels_prob=pseudo_labels_prob[1,:,:]\n",
        "\n",
        "        return pseudo_labels_prob\n",
        "\n",
        "    def estimate_label_frequency(self, y):\n",
        "        \"\"\"\n",
        "        estimate the label frequency empirically from the initial labeled data\n",
        "        Args:\n",
        "            y: label vector or matrix (multilabel)\n",
        "\n",
        "        Output:\n",
        "            Given K the number of labels, it returns a vector of label frequency [1 x K]\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        if self.IsMultiLabel==False:\n",
        "            if len(self.num_augmented_per_class)>0:\n",
        "                unique, label_frequency = np.unique( y[np.sum(self.num_augmented_per_class):], return_counts=True)\n",
        "            else:\n",
        "                unique, label_frequency = np.unique( y, return_counts=True)\n",
        "        else:\n",
        "            label_frequency = np.sum( y, axis=0)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"==label_frequency without adjustment\", np.round(label_frequency,3))\n",
        "\n",
        "        # smooth the label frequency if the ratio between the max class / min class is significant >5\n",
        "        # this smoothing is the implementation trick to prevent biased estimation given limited training data\n",
        "        ratio=np.max(label_frequency)/np.min(label_frequency)\n",
        "        if ratio>5:\n",
        "            label_frequency=label_frequency/np.sum(label_frequency)+np.ones( self.nClass )*1.0/self.nClass\n",
        "\n",
        "        return label_frequency/np.sum(label_frequency)\n",
        "\n",
        "\n",
        "\n",
        "    def evaluate_performance(self):\n",
        "        \"\"\"\n",
        "        evaluate_performance the classification performance\n",
        "        Store the result into: self.test_acc which is the accuracy for multiclassification \\\n",
        "                                                    or the precision for multilabel classification\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        y_test_pred = self.model.predict(self.x_test)\n",
        "\n",
        "        if self.IsMultiLabel==False:\n",
        "            test_acc= np.round( accuracy_score(y_test_pred, self.y_test)*100, 2)# round to 2 digits xx.yy %\n",
        "\n",
        "            if self.verbose:\n",
        "                print('+++Test Acc: {:.2f}%'.format(test_acc))\n",
        "            self.test_acc +=[test_acc]\n",
        "        else: # multi-label classification\n",
        "\n",
        "            # Precision\n",
        "            prec=sklearn.metrics.precision_score(self.y_test, y_test_pred,average='samples')*100\n",
        "            prec=np.round(prec,2) # round to 2 digits xx.yy %\n",
        "\n",
        "            self.test_acc +=[prec] # precision score\n",
        "\n",
        "            if self.verbose:\n",
        "                print('+++Test Acc: {:.2f}%'.format(prec))\n",
        "\n",
        "\n",
        "    def get_prob_at_max_class(self,pseudo_labels_prob):\n",
        "        \"\"\"\n",
        "        Given the 2d probability matrix [N x K], we get the probability at the maximum index\n",
        "        Args:\n",
        "           pseudo_labels_prob: 2d probability matrix [N x K]\n",
        "\n",
        "        Returns:\n",
        "           max_prob_matrix: probability at argmax class [N x 1]\n",
        "        \"\"\"\n",
        "        max_prob_matrix=np.zeros((pseudo_labels_prob.shape))\n",
        "        for ii in range(pseudo_labels_prob.shape[0]):  # loop over each data point\n",
        "            idxMax=np.argmax(pseudo_labels_prob[ii,:]) # find the highest score class\n",
        "            max_prob_matrix[ii,idxMax]=pseudo_labels_prob[ii,idxMax]\n",
        "        return max_prob_matrix\n",
        "\n",
        "    def post_processing_and_augmentation(self,assigned_pseudo_labels,X,y):\n",
        "        \"\"\"\n",
        "        after assigning the pseudo labels in the previous step, we post-process and augment them into X and y\n",
        "        Args:\n",
        "            assigned_pseudo_labels: [N x K] matrix where N is the #unlabels and K is the #class\n",
        "            assigned_pseudo_labels==0 indicates no assignment\n",
        "            assigned_pseudo_labels==1 indicates assignment.\n",
        "\n",
        "            X: existing pseudo_labeled + labeled data [ N' x d ]\n",
        "            y: existing pseudo_labeled + labeled data [ N' x 1 ] for multiclassification\n",
        "            y: existing pseudo_labeled + labeled data [ N' x K ] for multilabel classification\n",
        "        Output:\n",
        "            Augmented X\n",
        "            Augmented y\n",
        "        \"\"\"\n",
        "\n",
        "        sum_by_cols=np.sum(assigned_pseudo_labels,axis=1)\n",
        "        labels_satisfied_threshold = np.where(sum_by_cols>0)[0]\n",
        "\n",
        "        self.num_augmented_per_class.append( np.sum(assigned_pseudo_labels,axis=0).astype(int) )\n",
        "\n",
        "        if len(labels_satisfied_threshold) == 0: # no point is selected\n",
        "            return X,y\n",
        "\n",
        "        self.selected_unlabelled_index += labels_satisfied_threshold.tolist()\n",
        "\n",
        "        # augment the assigned labels to X and y ==============================================\n",
        "        X = np.vstack((self.unlabelled_data[labels_satisfied_threshold,:], X))\n",
        "\n",
        "        if self.IsMultiLabel==False: # y is [N x 1] matrix\n",
        "            # allow a single data point can be added into multiple\n",
        "            y = np.vstack(( np.argmax( assigned_pseudo_labels[labels_satisfied_threshold,:],axis=1).reshape(-1,1), np.array(y).reshape(-1,1)))\n",
        "\n",
        "        else: # y is [N x L] matrix\n",
        "            y = np.vstack((assigned_pseudo_labels[labels_satisfied_threshold,:], np.array(y)))\n",
        "\n",
        "\n",
        "        if \"CSA\" in self.algorithm_name: # book keeping\n",
        "            self.len_unlabels.append( len(self.unlabelled_data) )\n",
        "            self.len_accepted_ttest.append( assigned_pseudo_labels.shape[0] )\n",
        "            self.len_selected.append(  np.sum(self.num_augmented_per_class) )\n",
        "\n",
        "\n",
        "        # remove the selected data from unlabelled data\n",
        "        self.unlabelled_data = np.delete(self.unlabelled_data, np.unique(labels_satisfied_threshold), 0)\n",
        "\n",
        "        return X,y\n",
        "\n",
        "    def label_assignment_and_post_processing(self, pseudo_labels_prob,X,y, current_iter=0,upper_threshold=None):\n",
        "        \"\"\"\n",
        "        Given the threshold, we perform label assignment and post-processing\n",
        "\n",
        "        Args:\n",
        "            pseudo_labels_prob: predictive prob [N x K] where N is #unlabels, K is #class\n",
        "            X: existing pseudo_labeled + labeled data [ N' x d ]\n",
        "            y: existing pseudo_labeled + labeled data [ N' x 1 ] for multiclassification\n",
        "            y: existing pseudo_labeled + labeled data [ N' x K ] for multilabel classification\n",
        "\n",
        "        Output:\n",
        "            Augmented X = augmented_X + X\n",
        "            Augmented y = augmented_y + Y\n",
        "        \"\"\"\n",
        "\n",
        "        if self.IsMultiLabel==False:\n",
        "            #go over each row (data point), only keep the argmax prob\n",
        "            # because we only allow a single data point to a single class\n",
        "            max_prob_matrix=self.get_prob_at_max_class(pseudo_labels_prob)\n",
        "        else:\n",
        "            # we dont need to get prob at max class for multi-label\n",
        "            # because a single data point can be assigned to multiple classes\n",
        "            max_prob_matrix=pseudo_labels_prob\n",
        "\n",
        "\n",
        "        if upper_threshold is None:\n",
        "            upper_threshold=self.upper_threshold\n",
        "\n",
        "        if 'CSA' in self.algorithm_name: # if using CSA, we dont use the upper threshold\n",
        "            upper_threshold=0\n",
        "\n",
        "        assigned_pseudo_labels=np.zeros((max_prob_matrix.shape[0],self.nClass)).astype(int)\n",
        "\n",
        "        MaxPseudoPoint=[0]*self.nClass\n",
        "        for cc in range(self.nClass): # loop over each class\n",
        "\n",
        "            MaxPseudoPoint[cc]=self.get_max_pseudo_point(self.label_frequency[cc],current_iter)\n",
        "\n",
        "            idx_sorted = np.argsort( max_prob_matrix[:,cc])[::-1] # decreasing\n",
        "\n",
        "            temp_idx = np.where(max_prob_matrix[idx_sorted,cc] > upper_threshold )[0]\n",
        "            labels_satisfied_threshold=idx_sorted[temp_idx]\n",
        "\n",
        "            # only select upto MaxPseudoPoint[cc] points\n",
        "            labels_satisfied_threshold = labels_satisfied_threshold[:MaxPseudoPoint[cc]]\n",
        "            assigned_pseudo_labels[labels_satisfied_threshold, cc]=1\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"MaxPseudoPoint\",MaxPseudoPoint)\n",
        "\n",
        "        return self.post_processing_and_augmentation(assigned_pseudo_labels,X,y)\n",
        "\n",
        "\n",
        "\n",
        "    def get_number_of_labels(self,y):\n",
        "        \"\"\"\n",
        "        # given the label y, return the number of classes\n",
        "\n",
        "        Args:\n",
        "            y: label vector (for singlelabel) or matrix (for multilabel)\n",
        "\n",
        "        Output:\n",
        "            number of classes or number of labels\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        if self.IsMultiLabel==False:\n",
        "            return len(np.unique(y))\n",
        "        else:\n",
        "            return y.shape[1]\n",
        "\n",
        "\n",
        "\n",
        "    def get_max_pseudo_point(self,fraction_of_class, current_iter):\n",
        "        \"\"\"\n",
        "        We select more points at the begining and less at later stage\n",
        "\n",
        "        Args:\n",
        "            fraction_of_class: vector of the frequency of points per class\n",
        "            current_iter: current iteration  0,1,2...T\n",
        "        Output:\n",
        "            number_of_max_pseudo_points: scalar\n",
        "        \"\"\"\n",
        "\n",
        "        LinearRamp= [(self.num_iters-ii)/self.num_iters for ii in range(self.num_iters)]\n",
        "        SumLinearRamp=np.sum(LinearRamp)\n",
        "\n",
        "        fraction_iter= (self.num_iters-current_iter) / (self.num_iters*SumLinearRamp)\n",
        "        MaxPseudoPoint=fraction_iter*fraction_of_class*self.FractionAllocatedLabel*len(self.unlabelled_data)\n",
        "\n",
        "        return np.int(np.ceil(MaxPseudoPoint))\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        main algorithm to perform pseudo labelling\n",
        "\n",
        "        Args:\n",
        "            X: train features [N x d]\n",
        "            y: train targets [N x 1]\n",
        "\n",
        "        Output:\n",
        "            we record the test_accuracy a vector of test accuracy per pseudo-iteration\n",
        "        \"\"\"\n",
        "        print(\"=====\",self.algorithm_name)\n",
        "\n",
        "        self.nClass=self.get_number_of_labels(y)\n",
        "\n",
        "\n",
        "        self.label_frequency=self.estimate_label_frequency(y)\n",
        "\n",
        "        for current_iter in (tqdm(range(self.num_iters)) if self.verbose else range(self.num_iters)):\n",
        "            self.selected_unlabelled_index=[]\n",
        "\n",
        "            # Fit to data\n",
        "            self.model.fit(X, y)\n",
        "\n",
        "            # evaluate_performance the performance on test set after Fit the model given the data\n",
        "            self.evaluate_performance()\n",
        "\n",
        "            # Predictive probability on the unlabeled data\n",
        "            pseudo_labels_prob=self.get_predictive_prob_for_unlabelled_data(self.model)\n",
        "\n",
        "            X,y=self.label_assignment_and_post_processing(pseudo_labels_prob,X,y,current_iter)\n",
        "\n",
        "            if self.verbose:\n",
        "                print(\"#augmented:\", self.num_augmented_per_class, \" no training data \", len(y))\n",
        "\n",
        "            if np.sum(self.num_augmented_per_class)==0: # no data point is augmented\n",
        "                return\n",
        "\n",
        "        # evaluate_performance at the last iteration for reporting purpose\n",
        "        self.model.fit(X, y)\n",
        "\n",
        "        self.evaluate_performance()\n",
        "\n",
        "    # def predict(self, X):\n",
        "    #     return self.model.predict(X)\n",
        "\n",
        "    # def predict_proba(self, X):\n",
        "    #     return self.model.predict_proba(X)\n",
        "\n",
        "    # def decision_function(self, X):\n",
        "    #     return self.model.decision_function(X)"
      ],
      "metadata": {
        "id": "rdvd2_Uif3ew"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "import time\n",
        "\n",
        "# Confident Sinkhorn Allocation==================================================================================================\n",
        "class CSA(Pseudo_Labeling):\n",
        "\n",
        "    def __init__(self, unlabelled_data, x_test,y_test,num_iters=5,num_XGB_models=20,confidence_choice=\"ttest\",verbose = False,IsMultiLabel=False):\n",
        "        \"\"\"\n",
        "        unlabelled_data      : [N x d] where N is the number of unlabeled data, d is the feature dimension\n",
        "        x_test               :[N_test x d]\n",
        "        y_test               :[N_test x 1] for multiclassification or [N_test x K] for multilabel classification\n",
        "        num_iters            : number of pseudo-iterations, recommended = 5 as in the paper\n",
        "        upper_threshold      : the upper threshold used for pseudo-labeling, e.g., we assign label if the prob > 0.8\n",
        "        fraction_allocation  : the faction of label allocation, if fraction_allocation=1, we assign labels to 100% of unlabeled data\n",
        "        lower_threshold      : lower threshold, used for UPS\n",
        "        num_XGB_models       : number of XGB models used for UPS and CSA, recommended = 10\n",
        "        verbose              : verbose\n",
        "        IsMultiLabel         : False => Multiclassification or True => Multilabel classification\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__( unlabelled_data, x_test,y_test,num_iters=num_iters,num_XGB_models=num_XGB_models,verbose=verbose,IsMultiLabel=IsMultiLabel)\n",
        "\n",
        "\n",
        "        self.confidence_choice=confidence_choice\n",
        "\n",
        "        if self.IsMultiLabel==True:\n",
        "            # by default, we use total_variance as the main criteria for multilabel classification\n",
        "            if self.confidence_choice is not None:\n",
        "                self.confidence_choice=\"variance\"\n",
        "\n",
        "        if self.confidence_choice is None or self.confidence_choice==\"None\":\n",
        "            self.algorithm_name=\"SLA\"\n",
        "        else:\n",
        "            self.algorithm_name=\"CSA_\" + self.confidence_choice\n",
        "\n",
        "\n",
        "\n",
        "        self.elapse_xgb=[]\n",
        "        self.elapse_ttest=[]\n",
        "        self.elapse_sinkhorn=[]\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"number of used XGB models  M=\",self.num_XGB_models)\n",
        "\n",
        "    def predict(self, X):\n",
        "        super().predict(X)\n",
        "    def predict_proba(self, X):\n",
        "        super().predict_proba(X)\n",
        "    def evaluate_performance(self):\n",
        "        super().evaluate_performance()\n",
        "    def get_max_pseudo_point(self,class_freq,current_iter):\n",
        "        return super().get_max_pseudo_point(class_freq,current_iter)\n",
        "\n",
        "    def set_ot_regularizer(self,nRow,nCol):\n",
        "        \"\"\"\n",
        "        We set the Sinkhorn regularization parameter based on the ratio of Row/Column\n",
        "\n",
        "        Args:\n",
        "            nRow: number of rows in our cost matrix for Sinkhorn algorithm\n",
        "            nCol: number of columns\n",
        "\n",
        "        Output:\n",
        "            regularization\n",
        "        \"\"\"\n",
        "\n",
        "        if nRow/nCol>=300:\n",
        "            regulariser=1\n",
        "        if nRow/nCol>=200:\n",
        "            regulariser=0.5\n",
        "        elif nRow/nCol>=100:\n",
        "            regulariser=0.2\n",
        "        elif nRow/nCol>=50:\n",
        "            regulariser=0.1\n",
        "        else:\n",
        "            regulariser=0.05\n",
        "\n",
        "        if self.IsMultiLabel:\n",
        "            if self.nClass>20:\n",
        "                regulariser=regulariser*5\n",
        "            else:\n",
        "                regulariser=regulariser*200\n",
        "\n",
        "        return regulariser\n",
        "\n",
        "    def data_uncertainty(self,pseudo_labels_prob_list):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            pseudo_labels_prob_list: [M x N x K]\n",
        "        Output:\n",
        "            entropy: [N x 1]\n",
        "        \"\"\"\n",
        "\n",
        "        ent=np.zeros((pseudo_labels_prob_list.shape[0],pseudo_labels_prob_list.shape[1]))\n",
        "        for mm in range(pseudo_labels_prob_list.shape[0]):\n",
        "            ent[mm,:]= self.entropy_prediction(pseudo_labels_prob_list[mm,:,:])\n",
        "\n",
        "        return np.mean(ent,axis=0)\n",
        "\n",
        "    def entropy_prediction(self,ave_pred,atClass=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            ave_pred: [N x K]\n",
        "        Output:\n",
        "            entropy: [N x 1]\n",
        "        \"\"\"\n",
        "\n",
        "        ent=[0]*ave_pred.shape[0]\n",
        "\n",
        "        for ii in range(ave_pred.shape[0]):\n",
        "            ent[ii]= - np.sum( ave_pred[ii,:]*np.log(ave_pred[ii,:]))\n",
        "        return np.asarray(ent)\n",
        "\n",
        "    def total_entropy(self,pseudo_labels_prob_list, atClass=None):\n",
        "        \"\"\"\n",
        "        calculate total entropy\n",
        "        Args:\n",
        "            pseudo_labels_prob_list: [M x N x K]: M #XGB, N #unlabels, K #class\n",
        "        Output:\n",
        "            total_entropy score [N x 1]\n",
        "        \"\"\"\n",
        "\n",
        "        ave_pred=np.mean(pseudo_labels_prob_list,axis=0) # average over model\n",
        "\n",
        "        total_uncertainty=self.entropy_prediction(ave_pred,atClass)\n",
        "        return total_uncertainty\n",
        "\n",
        "    def knowledge_uncertainty(self,pred):\n",
        "\n",
        "        total_uncertainty=self.total_uncertainty(pred)\n",
        "\n",
        "        data_uncertainty=self.data_uncertainty(pred)\n",
        "\n",
        "        knowledge_uncertainty = total_uncertainty-data_uncertainty\n",
        "        return knowledge_uncertainty\n",
        "\n",
        "    def total_variance(self,pseudo_labels_prob_list):\n",
        "        \"\"\"\n",
        "        calculate total variance\n",
        "        Args:\n",
        "            pseudo_labels_prob_list: [M x N x K]: M #XGB, N #unlabels, K #class\n",
        "        Output:\n",
        "            standard deviation score [N x 1]\n",
        "        \"\"\"\n",
        "\n",
        "        # [nModel, nPoint, nClass]\n",
        "        std_pred = np.std( pseudo_labels_prob_list, axis=0) # std over models\n",
        "        total_std = np.sum(std_pred, axis=1) # sum of std over classes\n",
        "\n",
        "        return total_std\n",
        "\n",
        "    def calculate_ttest(self,pseudo_labels_prob_list):\n",
        "        \"\"\"\n",
        "        calculate t-test\n",
        "        Args:\n",
        "            pseudo_labels_prob_list: [M x N x K]: M #XGB, N #unlabels, K #class\n",
        "        Output:\n",
        "            t-test score [N x 1]\n",
        "        \"\"\"\n",
        "\n",
        "        num_points=pseudo_labels_prob_list.shape[1]\n",
        "\n",
        "        var_rows_argmax=[0]*num_points\n",
        "        var_rows_arg2ndmax=[0]*num_points\n",
        "\n",
        "        t_test=[0]*num_points\n",
        "        t_value=[0]*num_points\n",
        "\n",
        "\n",
        "        pseudo_labels_prob= np.mean(pseudo_labels_prob_list,axis=0)\n",
        "\n",
        "        temp=np.argsort(-pseudo_labels_prob,axis=1) # decreasing\n",
        "        idxargmax=temp[:,0]\n",
        "        idx2nd_argmax= temp[:,1]\n",
        "\n",
        "        for jj in range(num_points):# go over each row (data points)\n",
        "\n",
        "            idxmax =idxargmax[jj]\n",
        "            idx2ndmax=idx2nd_argmax[jj]\n",
        "\n",
        "            var_rows_argmax[jj]=np.var(pseudo_labels_prob_list[:,jj,idxmax  ])\n",
        "            var_rows_arg2ndmax[jj]=np.var(pseudo_labels_prob_list[:,jj,idx2ndmax])\n",
        "\n",
        "            nominator=pseudo_labels_prob[jj, idxmax]-pseudo_labels_prob[jj, idx2ndmax]\n",
        "            temp=(0.1 + var_rows_argmax[jj] + var_rows_arg2ndmax[jj]  )/self.num_XGB_models\n",
        "            denominator=np.sqrt(temp)\n",
        "            t_test[jj] = nominator/denominator\n",
        "\n",
        "            # compute degree of freedom=========================================\n",
        "            nominator = (var_rows_argmax[jj] + var_rows_arg2ndmax[jj])**2\n",
        "\n",
        "            denominator= var_rows_argmax[jj]**2 + var_rows_arg2ndmax[jj]**2\n",
        "            denominator=denominator/(self.num_XGB_models-1)\n",
        "            dof=nominator/denominator\n",
        "\n",
        "            t_value[jj]=stats.t.ppf(1-0.025, dof)\n",
        "\n",
        "            t_test[jj]=t_test[jj]-t_value[jj]\n",
        "\n",
        "        return t_test\n",
        "\n",
        "    def label_assignment_and_post_processing_for_CSA(self, assignment_matrix,pseudo_labels_prob,X,y, current_iter=0):\n",
        "        \"\"\"\n",
        "        Given the threshold, we perform label assignment and post-processing\n",
        "\n",
        "        Args:\n",
        "            pseudo_labels_prob: predictive prob [N x K] where N is #unlabels, K is #class\n",
        "            X: existing pseudo_labeled + labeled data [ N' x d ]\n",
        "            y: existing pseudo_labeled + labeled data [ N' x 1 ] for multiclassification\n",
        "            y: existing pseudo_labeled + labeled data [ N' x K ] for multilabel classification\n",
        "\n",
        "        Output:\n",
        "            Augmented X = augmented_X + X\n",
        "            Augmented y = augmented_y + Y\n",
        "        \"\"\"\n",
        "\n",
        "        if self.IsMultiLabel==False:\n",
        "            #go over each row (data point), only keep the argmax prob\n",
        "            # because we only allow a single data point to a single class\n",
        "            max_prob_matrix=self.get_prob_at_max_class(pseudo_labels_prob)\n",
        "        else:\n",
        "            # we dont need to get prob at max class for multi-label\n",
        "            # because a single data point can be assigned to multiple classes\n",
        "            max_prob_matrix=pseudo_labels_prob\n",
        "\n",
        "        assignment_matrix=self.get_prob_at_max_class(assignment_matrix)\n",
        "\n",
        "        assigned_pseudo_labels=np.zeros((max_prob_matrix.shape[0],self.nClass)).astype(int)\n",
        "\n",
        "        MaxPseudoPoint=[0]*self.nClass\n",
        "        for cc in range(self.nClass): # loop over each class\n",
        "\n",
        "            MaxPseudoPoint[cc]=self.get_max_pseudo_point(self.label_frequency[cc],current_iter)\n",
        "\n",
        "            idx_sorted = np.argsort( assignment_matrix[:,cc])[::-1] # decreasing\n",
        "\n",
        "            idx_assignment = np.where(assignment_matrix[idx_sorted,cc] > 0 )[0]\n",
        "\n",
        "            # we dont accept labels with less than 0.5 prediction, this works well for multilabel classification\n",
        "            idx_satisfied = np.where(pseudo_labels_prob[idx_sorted[idx_assignment],cc] > 0.5 )[0]\n",
        "\n",
        "            # only select upto MaxPseudoPoint[cc] points\n",
        "            labels_satisfied_threshold=idx_sorted[idx_satisfied][:MaxPseudoPoint[cc]]\n",
        "\n",
        "            assigned_pseudo_labels[labels_satisfied_threshold, cc]=1\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"MaxPseudoPoint\",MaxPseudoPoint)\n",
        "\n",
        "        return self.post_processing_and_augmentation(assigned_pseudo_labels,X,y)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        main algorithm to perform pseudo labelling\n",
        "\n",
        "        Args:\n",
        "            X: train features [N x d]\n",
        "            y: train targets [N x 1]\n",
        "\n",
        "        Output:\n",
        "            we record the test_accuracy a vector of test accuracy per pseudo-iteration\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"=====\",self.algorithm_name)\n",
        "\n",
        "        self.nClass=self.get_number_of_labels(y)\n",
        "\n",
        "        self.label_frequency=self.estimate_label_frequency(y)\n",
        "\n",
        "        for current_iter in (tqdm(range(self.num_iters)) if self.verbose else range(self.num_iters)):\n",
        "\n",
        "            # Fit to data\n",
        "            self.model.fit(X, y)\n",
        "\n",
        "            self.evaluate_performance()\n",
        "\n",
        "\n",
        "            num_points=self.unlabelled_data.shape[0]\n",
        "            pseudo_labels_prob_list=[0]*self.num_XGB_models\n",
        "\n",
        "            tic = time.perf_counter()\n",
        "\n",
        "            # estimate prob using unlabelled data on M XGB models\n",
        "            pseudo_labels_prob_list=[0]*self.num_XGB_models\n",
        "            for mm in range(self.num_XGB_models):\n",
        "                self.XGBmodels_list[mm].fit(X, y)\n",
        "                pseudo_labels_prob_list[mm] = self.get_predictive_prob_for_unlabelled_data(self.XGBmodels_list[mm])\n",
        "\n",
        "\n",
        "            toc = time.perf_counter()\n",
        "            self.elapse_xgb.append(toc-tic)\n",
        "\n",
        "            pseudo_labels_prob_list=np.asarray(pseudo_labels_prob_list) # P [M x N x K]\n",
        "            pseudo_labels_prob= np.mean(pseudo_labels_prob_list,axis=0) # \\bar{P} [N x K]\n",
        "\n",
        "            tic = time.perf_counter() # Start Time\n",
        "\n",
        "\n",
        "            # estimate confidence level here====================================\n",
        "            if self.confidence_choice==\"variance\":\n",
        "                tot_variance=self.total_variance(pseudo_labels_prob_list)\n",
        "                confidence=1-tot_variance\n",
        "                confidence=confidence-np.mean(confidence)\n",
        "            elif self.confidence_choice==\"neg_variance\":\n",
        "                confidence=self.total_variance(pseudo_labels_prob_list)\n",
        "                confidence=confidence-np.mean(confidence)\n",
        "            elif self.confidence_choice=='entropy':\n",
        "                tot_ent=self.total_entropy(pseudo_labels_prob_list)\n",
        "                confidence=1-tot_ent\n",
        "                confidence=confidence-0.5*np.mean(confidence)\n",
        "            elif self.confidence_choice=='neg_entropy':\n",
        "                confidence=self.total_entropy(pseudo_labels_prob_list)\n",
        "                confidence=confidence-np.mean(confidence)\n",
        "\n",
        "            elif self.confidence_choice==\"ttest\":\n",
        "                confidence=self.calculate_ttest(pseudo_labels_prob_list)\n",
        "            elif self.confidence_choice==\"neg_ttest\":\n",
        "                confidence=self.calculate_ttest(pseudo_labels_prob_list)\n",
        "                confidence=-np.asarray(confidence)\n",
        "            elif self.confidence_choice==None or self.confidence_choice==\"None\":  # not using any confidence score, accepting all data point similar to SLA\n",
        "                confidence=np.ones((1,num_points))\n",
        "\n",
        "            confidence=np.clip(confidence, a_min=0,a_max=np.max(confidence))\n",
        "\n",
        "            toc = time.perf_counter() # End Time\n",
        "            self.elapse_ttest.append(toc-tic)\n",
        "\n",
        "            # for numerical stability of OT, select the nonzero entry only\n",
        "            idxNoneZero=np.where( confidence>0 )[0]\n",
        "            #idxNoneZero=np.where( (confidence>0) & (confidence<0.9*np.max(confidence)) )[0]\n",
        "            num_points= len(idxNoneZero)\n",
        "\n",
        "            if self.verbose:\n",
        "                print(\"num_points accepted= \",num_points, \" total num_points=\",len(self.unlabelled_data))\n",
        "\n",
        "            if len(idxNoneZero)==0: # terminate if could not find any point satisfying constraints\n",
        "                return self.test_acc\n",
        "\n",
        "            # Sinkhorn's algorithm ======================================================================\n",
        "            # fraction of label being assigned.\n",
        "            max_allocation_point= self.get_max_pseudo_point(class_freq=1,current_iter=current_iter)\n",
        "            rho=max_allocation_point/ len(self.unlabelled_data)\n",
        "\n",
        "            # regulariser for Sinkhorn's algorithm\n",
        "            regulariser=self.set_ot_regularizer(num_points, self.nClass)\n",
        "\n",
        "            tic = time.perf_counter()\n",
        "\n",
        "\n",
        "            # this is w_{+} and w_{-} in the paper\n",
        "            upper_b_per_class=self.label_frequency*1.1\n",
        "            lower_b_per_class=self.label_frequency*0.9\n",
        "\n",
        "            # we define row marginal distribution =============================\n",
        "            row_marginal=np.ones(num_points)\n",
        "            temp=num_points*rho*(np.sum(upper_b_per_class)-np.sum(lower_b_per_class))\n",
        "            row_marginal = np.append(row_marginal,temp)\n",
        "\n",
        "            if self.verbose:\n",
        "                print(\"#unlabel={:d} #points/#classes={:d}/{:d}={:.2f} reg={:.2f}\".format(\n",
        "                    len(self.unlabelled_data),num_points,self.nClass,num_points/self.nClass,regulariser))\n",
        "\n",
        "\n",
        "            C=1-pseudo_labels_prob # cost # expand Cost matrix\n",
        "            C=C[idxNoneZero,:]\n",
        "\n",
        "            C=np.vstack((C,np.zeros((1,self.nClass))))\n",
        "            C=np.hstack((C,np.zeros((len(idxNoneZero)+1,1))))\n",
        "\n",
        "            K=np.exp(-C/regulariser)\n",
        "\n",
        "            # define column marginal distribution ==============================\n",
        "            col_marginal = rho*upper_b_per_class*num_points  # frequency of the class label\n",
        "            temp=num_points*(1-rho*np.sum(lower_b_per_class))\n",
        "            col_marginal = np.append(col_marginal,temp)\n",
        "\n",
        "            # checking the total mass of column marginal ~ row marginal\n",
        "            if np.abs( np.sum(col_marginal) - np.sum(row_marginal) ) > 0.001 :\n",
        "                print(\"np.sum(dist_labels) - np.sum(dist_points) > 0.001\")\n",
        "\n",
        "            # initialize uu and perform Sinkhorn algorithm\n",
        "            uu=np.ones( (num_points+1,))\n",
        "            for jj in range(100):\n",
        "                vv= col_marginal / np.dot(K.T, uu)\n",
        "                uu= row_marginal / np.dot(K, vv)\n",
        "\n",
        "\n",
        "            # compute label assignment matrix Q'\n",
        "            Q_prime= np.atleast_2d(uu).T*(K*vv.T)\n",
        "\n",
        "            toc = time.perf_counter()\n",
        "            self.elapse_sinkhorn.append(toc-tic)\n",
        "\n",
        "            # this is the final Q matrix\n",
        "            assignment_matrix_Q=np.zeros((pseudo_labels_prob.shape))\n",
        "            assignment_matrix_Q[idxNoneZero,:]=Q_prime[:-1,:-1]\n",
        "\n",
        "            X,y=self.label_assignment_and_post_processing_for_CSA(assignment_matrix_Q,pseudo_labels_prob,X,y,current_iter)\n",
        "\n",
        "            if self.verbose:\n",
        "                print(\"#augmented:\", self.num_augmented_per_class, \" len of training data \", len(y))\n",
        "\n",
        "\n",
        "        # evaluate_performance at the last iteration for reporting purpose\n",
        "        self.model.fit(X, y)\n",
        "\n",
        "        self.evaluate_performance()"
      ],
      "metadata": {
        "id": "3oJUe4mVfgOC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name='digits'\n",
        "path_to_file='./all_data.pickle'\n",
        "x_train,y_train, x_test, y_test, x_unlabeled=get_train_test_unlabeled(dataset_name,path_to_file,random_state=0)"
      ],
      "metadata": {
        "id": "XvGmtkLRf7Bh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yt = []\n",
        "for yy in y_train:\n",
        "  yt.append(yy[0])\n",
        "\n",
        "print(set(yt))\n",
        "\n",
        "ytt = []\n",
        "for yy in y_test:\n",
        "  ytt.append(yy[0])\n",
        "\n",
        "print(set(ytt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdIeNVE0gOFF",
        "outputId": "b013faac-cc96-4137-ae7f-b617ce2fcfce"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0}\n",
            "{0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "print(\"number of training points = \",y_train.shape[0])\n",
        "\n",
        "print(\"number of test points = \",y_test.shape[0])\n",
        "\n",
        "print(\"number of unlabelled points = \",x_unlabeled.shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaanhK6XK-or",
        "outputId": "475169c0-ba08-42bd-fc4c-87ac6d3ce958"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(143, 64)\n",
            "(143, 1)\n",
            "number of training points =  143\n",
            "number of test points =  360\n",
            "number of unlabelled points =  1294\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Purely Supervised learning\n",
        "# Train an XGBoost model using on training data (labelled data)"
      ],
      "metadata": {
        "id": "sXB8pJOZLOBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this is the XGBoost model for multi-class classification\n",
        "param = {}\n",
        "param['booster'] = 'gbtree'\n",
        "param['objective'] = 'binary:logistic'\n",
        "param['verbosity'] = 0\n",
        "param['silent'] = 1\n",
        "param['seed'] = 0\n",
        "\n",
        "# create XGBoost instance with default hyper-parameters\n",
        "xgb=XGBClassifier(**param,use_label_encoder=False)\n",
        "\n",
        "xgb.fit(x_train, y_train)\n",
        "\n",
        "# evaluate the performance on the test set\n",
        "y_test_pred = xgb.predict(x_test)\n",
        "supervised_learning_accuracy= np.round( accuracy_score(y_test_pred, y_test)*100, 2)# round to 2 digits xx.yy %\n",
        "\n",
        "print('+++Test Acc: {:.2f}%'.format(supervised_learning_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4QK8824LLJh",
        "outputId": "73711ad4-3050-4d3d-d59d-5a6dcf4937b3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+++Test Acc: 75.28%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform Semi-supervised learning using Pseudo-labeling\n",
        "# Set a few hyperparameters for the model"
      ],
      "metadata": {
        "id": "N1okHgdkLSBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numTrials=1\n",
        "numIters=5\n",
        "upper_threshold=0.8\n",
        "dataset_name='digits'"
      ],
      "metadata": {
        "id": "V2eF6fh4LQUp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pseudo_labeling_model = Pseudo_Labeling(x_unlabeled,x_test,y_test,\n",
        "                num_iters=numIters,\n",
        "                upper_threshold=upper_threshold,\n",
        "                verbose = True)\n",
        "\n",
        "pseudo_labeling_model.fit(x_train, y_train)\n",
        "\n",
        "pseudo_labeling_accuracy=pseudo_labeling_model.test_acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0gCYfpOLUDM",
        "outputId": "6335641f-07fe-46ff-d0c8-75ccf31da252"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no of unlabelled data: 1294 \t no of test data: 360\n",
            "===== Pseudo_Labeling\n",
            "==label_frequency without adjustment [22 20 16 13 18 11 17 12  6  8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 1/5 [00:01<00:06,  1.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+++Test Acc: 75.28%\n",
            "MaxPseudoPoint [67, 61, 49, 40, 55, 34, 52, 37, 19, 25]\n",
            "#augmented: [array([67, 61, 49, 40, 55, 34, 52, 37, 19, 25])]  no training data  582\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 2/5 [00:03<00:05,  1.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+++Test Acc: 75.56%\n",
            "MaxPseudoPoint [36, 32, 26, 21, 29, 18, 28, 20, 10, 13]\n",
            "#augmented: [array([67, 61, 49, 40, 55, 34, 52, 37, 19, 25]), array([36, 22, 26, 21, 29, 18, 28, 20, 10, 13])]  no training data  805\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 3/5 [00:07<00:05,  2.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+++Test Acc: 77.50%\n",
            "MaxPseudoPoint [20, 18, 15, 12, 16, 10, 16, 11, 6, 8]\n",
            "#augmented: [array([67, 61, 49, 40, 55, 34, 52, 37, 19, 25]), array([36, 22, 26, 21, 29, 18, 28, 20, 10, 13]), array([20, 16, 15, 12, 16, 10, 16, 11,  6,  8])]  no training data  935\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 4/5 [00:10<00:02,  2.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+++Test Acc: 76.39%\n",
            "MaxPseudoPoint [11, 10, 8, 7, 9, 6, 8, 6, 3, 4]\n",
            "#augmented: [array([67, 61, 49, 40, 55, 34, 52, 37, 19, 25]), array([36, 22, 26, 21, 29, 18, 28, 20, 10, 13]), array([20, 16, 15, 12, 16, 10, 16, 11,  6,  8]), array([11,  1,  8,  7,  9,  6,  8,  6,  3,  4])]  no training data  998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:15<00:00,  3.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+++Test Acc: 77.50%\n",
            "MaxPseudoPoint [5, 5, 4, 3, 4, 3, 4, 3, 2, 2]\n",
            "#augmented: [array([67, 61, 49, 40, 55, 34, 52, 37, 19, 25]), array([36, 22, 26, 21, 29, 18, 28, 20, 10, 13]), array([20, 16, 15, 12, 16, 10, 16, 11,  6,  8]), array([11,  1,  8,  7,  9,  6,  8,  6,  3,  4]), array([5, 1, 4, 3, 4, 3, 4, 3, 2, 2])]  no training data  1029\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+++Test Acc: 78.61%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# # Plot the comparison between Supervised Learning vs Pseudo-labeling"
      ],
      "metadata": {
        "id": "k3FkaOyVLXw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!python -m pip uninstall matplotlib\n",
        "#!pip install matplotlib==3.1.3\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# plot in the same axis\n",
        "\n",
        "supervised_learning_result=[ supervised_learning_accuracy ]*len(pseudo_labeling_accuracy)\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.arange(len(pseudo_labeling_accuracy)),\\\n",
        "         supervised_learning_result,'m:',linewidth=4,label=\"Supervised Learning\")\n",
        "plt.plot(pseudo_labeling_accuracy,'k-.',linewidth=4,label='Pseudo-labeling')\n",
        "\n",
        "plt.xlabel(\"Pseudo-labeling Iteration\",fontsize=14)\n",
        "plt.ylabel(\"Test Accuracy\",fontsize=14)\n",
        "\n",
        "plt.legend(fontsize=14)\n",
        "\n",
        "plt.title(\"Dataset = \" + dataset_name,fontsize=14 )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "id": "KtHe9m9sLWV2",
        "outputId": "2156470b-d8e4-4fa7-8a8a-8190b95d143d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Dataset = digits')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the proposed Confident Sinkhorn Allocation (CSA)"
      ],
      "metadata": {
        "id": "V30nYdLQLbUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_XGB_models=10\n",
        "confidence_choice='ttest'\n",
        "\n",
        "csa_model = CSA(x_unlabeled,x_test,y_test,\n",
        "                num_iters=numIters,\n",
        "                confidence_choice=confidence_choice,\n",
        "                num_XGB_models=num_XGB_models,\n",
        "                verbose = True,\n",
        "            )\n",
        "csa_model.fit(x_train, y_train)\n",
        "\n",
        "csa_accuracy=csa_model.test_acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okEcUxujLdNE",
        "outputId": "06e84f1a-139a-4d7a-81d6-8950f9592529"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no of unlabelled data: 1294 \t no of test data: 360\n",
            "number of used XGB models  M= 10\n",
            "===== CSA_ttest\n",
            "==label_frequency without adjustment [22 20 16 13 18 11 17 12  6  8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+++Test Acc: 75.28%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 1/5 [00:03<00:14,  3.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_points accepted=  1014  total num_points= 1294\n",
            "#unlabel=1294 #points/#classes=1014/10=101.40 reg=0.20\n",
            "MaxPseudoPoint [67, 61, 49, 40, 55, 34, 52, 37, 19, 25]\n",
            "#augmented: [array([67, 61, 49, 40, 55, 34, 52, 37, 19, 25])]  len of training data  582\n",
            "+++Test Acc: 76.67%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 2/5 [00:11<00:18,  6.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_points accepted=  636  total num_points= 855\n",
            "#unlabel=855 #points/#classes=636/10=63.60 reg=0.10\n",
            "MaxPseudoPoint [36, 32, 26, 21, 29, 18, 28, 20, 10, 13]\n",
            "#augmented: [array([67, 61, 49, 40, 55, 34, 52, 37, 19, 25]), array([36, 32, 26, 21, 29, 18, 28, 20, 10, 13])]  len of training data  815\n",
            "+++Test Acc: 78.33%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 3/5 [00:19<00:14,  7.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_points accepted=  426  total num_points= 622\n",
            "#unlabel=622 #points/#classes=426/10=42.60 reg=0.05\n",
            "MaxPseudoPoint [20, 18, 14, 12, 16, 10, 15, 11, 6, 7]\n",
            "#augmented: [array([67, 61, 49, 40, 55, 34, 52, 37, 19, 25]), array([36, 32, 26, 21, 29, 18, 28, 20, 10, 13]), array([20, 18, 14, 12, 16, 10, 15, 11,  6,  7])]  len of training data  944\n",
            "+++Test Acc: 79.44%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 4/5 [00:26<00:06,  6.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_points accepted=  318  total num_points= 493\n",
            "#unlabel=493 #points/#classes=318/10=31.80 reg=0.05\n",
            "MaxPseudoPoint [11, 10, 8, 6, 9, 6, 8, 6, 3, 4]\n",
            "#augmented: [array([67, 61, 49, 40, 55, 34, 52, 37, 19, 25]), array([36, 32, 26, 21, 29, 18, 28, 20, 10, 13]), array([20, 18, 14, 12, 16, 10, 15, 11,  6,  7]), array([7, 9, 8, 6, 9, 6, 8, 6, 3, 4])]  len of training data  1010\n",
            "+++Test Acc: 79.44%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:35<00:00,  7.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_points accepted=  261  total num_points= 427\n",
            "#unlabel=427 #points/#classes=261/10=26.10 reg=0.05\n",
            "MaxPseudoPoint [5, 4, 4, 3, 4, 3, 4, 3, 2, 2]\n",
            "#augmented: [array([67, 61, 49, 40, 55, 34, 52, 37, 19, 25]), array([36, 32, 26, 21, 29, 18, 28, 20, 10, 13]), array([20, 18, 14, 12, 16, 10, 15, 11,  6,  7]), array([7, 9, 8, 6, 9, 6, 8, 6, 3, 4]), array([3, 4, 4, 3, 4, 3, 4, 3, 2, 2])]  len of training data  1042\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+++Test Acc: 80.28%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot the comparison between Supervised, Pseudo-labeling and CSA"
      ],
      "metadata": {
        "id": "rlXWgnauLew0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot in the same axis\n",
        "\n",
        "supervised_learning_result=[ supervised_learning_accuracy ]*len(pseudo_labeling_accuracy)\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "\n",
        "# Supervised Learning\n",
        "plt.plot(np.arange(len(pseudo_labeling_accuracy)),\\\n",
        "         supervised_learning_result,'m:',linewidth=4,label=\"Supervised Learning\")\n",
        "\n",
        "# Pseudo Labeling\n",
        "plt.plot(pseudo_labeling_accuracy,'k-.',linewidth=4,label='Pseudo-labeling')\n",
        "\n",
        "# CSA\n",
        "plt.plot(csa_accuracy,'r-',linewidth=4,label='CSA')\n",
        "\n",
        "plt.xlabel(\"Pseudo-labeling Iteration\",fontsize=14)\n",
        "plt.ylabel(\"Test Accuracy\",fontsize=14)\n",
        "\n",
        "plt.legend(fontsize=12)\n",
        "\n",
        "plt.title(\"Dataset = \" + dataset_name,fontsize=14 )"
      ],
      "metadata": {
        "id": "TKbNRJYoLe6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# # Run ablation study of CSA without ttest, using all data points, like the SLA paper"
      ],
      "metadata": {
        "id": "TARx06QGLhlb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "confidence_choice=\"none\"\n",
        "\n",
        "# SLA model ~ CSA without ttest\n",
        "\n",
        "sla_model = CSA(x_unlabeled,x_test,y_test,\n",
        "                num_iters=numIters,\n",
        "                confidence_choice=confidence_choice,\n",
        "                num_XGB_models=num_XGB_models,\n",
        "                verbose = True,\n",
        "            )\n",
        "sla_model.fit(x_train, y_train)\n",
        "\n",
        "sla_accuracy=sla_model.test_acc"
      ],
      "metadata": {
        "id": "tHiGJ-z1Ljad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot in the same axis\n",
        "\n",
        "supervised_learning_result=[ supervised_learning_accuracy ]*len(pseudo_labeling_accuracy)\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "\n",
        "# Supervised Learning\n",
        "plt.plot(np.arange(len(pseudo_labeling_accuracy)),\\\n",
        "         supervised_learning_result,'m:',linewidth=4,label=\"Supervised Learning\")\n",
        "\n",
        "# Pseudo Labeling\n",
        "plt.plot(pseudo_labeling_accuracy,'k-.',linewidth=4,label='Pseudo-labeling')\n",
        "\n",
        "# SLA\n",
        "plt.plot(sla_accuracy,'b:',linewidth=4,label='SLA')\n",
        "\n",
        "# CSA\n",
        "plt.plot(csa_accuracy,'r-',linewidth=4,label='CSA')\n",
        "\n",
        "plt.xlabel(\"Pseudo-labeling Iteration\",fontsize=14)\n",
        "plt.ylabel(\"Test Accuracy\",fontsize=14)\n",
        "\n",
        "plt.legend(fontsize=12)\n",
        "\n",
        "plt.title(\"Dataset = \" + dataset_name,fontsize=14 )"
      ],
      "metadata": {
        "id": "CUvdaf0VLtMV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}